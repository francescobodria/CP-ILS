{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from scipy.spatial.distance import hamming, euclidean, cdist\n",
    "\n",
    "import torch\n",
    "random_seed = 42\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "dataset_name = 'compas'\n",
    "# number of counterfactuals to test\n",
    "n = 100\n",
    "# also train the latent space, set to false if already trained\n",
    "latent_train = True\n",
    "# Latent Space Parameters\n",
    "latent_dim = 3\n",
    "batch_size = 1024\n",
    "sigma = 1\n",
    "max_epochs = 1000\n",
    "early_stopping = 3\n",
    "learning_rate = 1e-3\n",
    "if dataset_name == 'adult':\n",
    "    idx_cat = [2,3,4,5,6]\n",
    "elif dataset_name == 'fico':\n",
    "    idx_cat = None\n",
    "elif dataset_name == 'german':\n",
    "    idx_cat = np.arange(3,71,1).tolist()\n",
    "elif dataset_name == 'compas':\n",
    "    idx_cat = list(range(13,33,1))\n",
    "# elif dataset_name == 'diva':\n",
    "#     idx_cat = list(range(58))\n",
    "\n",
    "# LOAD Dataset\n",
    "from exp.data_loader import load_tabular_data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, df = load_tabular_data(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4936, 33])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(X_train.values).float().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4936, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(Y_train.values).reshape(-1,1).int().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /Users/francescobodria/Github/CP-ILS/lightning_logs\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | fc1  | Linear           | 340   \n",
      "1 | fc2  | Linear           | 55    \n",
      "2 | out  | Linear           | 6     \n",
      "3 | loss | CrossEntropyLoss | 0     \n",
      "------------------------------------------\n",
      "401       Trainable params\n",
      "0         Non-trainable params\n",
      "401       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea531e03f17944948fc16f05818d2c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class Data(pl.LightningDataModule):\n",
    "    def prepare_data(self):      \n",
    "        self.train_data = TensorDataset(torch.tensor(X_train.values).float(),torch.tensor(Y_train.values).reshape(-1,1).int())\n",
    "        self.test_data = TensorDataset(torch.tensor(X_test.values).float(),torch.tensor(Y_test.values).reshape(-1,1).int())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=1024, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=1024, shuffle=False)\n",
    "\n",
    "class FFNN(pl.LightningModule):\n",
    "    def __init__(self, input_shape):\n",
    "        super(FFNN,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,10)\n",
    "        self.fc2 = nn.Linear(10,5)\n",
    "        self.out = nn.Linear(5,1)\n",
    "        self.lr = 1e-3\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # batch_size, _, _, _ = x.size()\n",
    "        # x = x.view(batch_size,-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits,y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, valid_batch, batch_idx):\n",
    "        x, y = valid_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits,y)\n",
    "\n",
    "# Create Model Object\n",
    "clf_nn = FFNN(X_train.shape[1])\n",
    "# Create Data Module Object\n",
    "data = Data()\n",
    "# Create Trainer Object\n",
    "trainer = pl.Trainer(max_epochs=500)\n",
    "trainer.fit(clf_nn,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB\n",
      "train acc: 0.9874392220421394\n",
      "test acc: 0.9530364372469635\n",
      "RF\n",
      "train acc: 1.0\n",
      "test acc: 0.9489878542510122\n",
      "SVC\n",
      "train acc: 0.9325364667747164\n",
      "test acc: 0.934412955465587\n"
     ]
    }
   ],
   "source": [
    "# load Black Boxes\n",
    "\n",
    "# XGB\n",
    "from xgboost import XGBClassifier\n",
    "clf_xgb = XGBClassifier(n_estimators=60, reg_lambda=3, use_label_encoder=False, eval_metric='logloss')\n",
    "clf_xgb.fit(X_train.values, Y_train.values)\n",
    "#clf_xgb.save_model(f'./blackboxes/{dataset_name}_xgboost')\n",
    "#clf_xgb.load_model(f'./blackboxes/{dataset_name}_xgboost')\n",
    "y_train_pred = clf_xgb.predict(X_train.values)\n",
    "y_test_pred = clf_xgb.predict(X_test.values)\n",
    "print('XGB')\n",
    "print('train acc:',np.mean(np.round(y_train_pred)==Y_train))\n",
    "print('test acc:',np.mean(np.round(y_test_pred)==Y_test))\n",
    "\n",
    "# RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(random_state=random_seed)\n",
    "clf_rf.fit(X_train, Y_train)\n",
    "pickle.dump(clf_rf,open(f'./blackboxes/{dataset_name}_rf.p','wb'))\n",
    "clf_rf = pickle.load(open(f'./blackboxes/{dataset_name}_rf.p','rb'))\n",
    "y_train_pred = clf_rf.predict(X_train)\n",
    "y_test_pred = clf_rf.predict(X_test)\n",
    "print('RF')\n",
    "print('train acc:',np.mean(np.round(y_train_pred)==Y_train))\n",
    "print('test acc:',np.mean(np.round(y_test_pred)==Y_test))\n",
    "\n",
    "# SVC\n",
    "from sklearn.svm import SVC\n",
    "clf_svc = SVC(gamma='auto', probability=True)\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "pickle.dump(clf_svc,open(f'./blackboxes/{dataset_name}_svc.p','wb'))\n",
    "clf_svc = pickle.load(open(f'./blackboxes/{dataset_name}_svc.p','rb'))\n",
    "y_train_pred = clf_svc.predict(X_train)\n",
    "y_test_pred = clf_svc.predict(X_test)\n",
    "print('SVC')\n",
    "print('train acc:',np.mean(np.round(y_train_pred)==Y_train))\n",
    "print('test acc:',np.mean(np.round(y_test_pred)==Y_test))\n",
    "\n",
    "# NN\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# BATCH_SIZE = 1024\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(2048).batch(BATCH_SIZE)\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(BATCH_SIZE)\n",
    "# clf_nn = keras.Sequential([\n",
    "#     keras.layers.Dense(units=10, activation='relu'),\n",
    "#     keras.layers.Dense(units=5, activation='relu'),\n",
    "#     keras.layers.Dense(units=1, activation='sigmoid'),\n",
    "# ])\n",
    "# early_stopping = EarlyStopping(patience=5)\n",
    "# clf_nn.compile(optimizer='adam', \n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# history = clf_nn.fit(\n",
    "#     train_dataset,\n",
    "#     validation_data=test_dataset,\n",
    "#     epochs=500,\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=0\n",
    "# )\n",
    "# def plot_metric(history, metric):\n",
    "#     train_metrics = history.history[metric]\n",
    "#     val_metrics = history.history['val_'+metric]\n",
    "#     epochs = range(1, len(train_metrics) + 1)\n",
    "#     plt.plot(epochs, train_metrics)\n",
    "#     plt.plot(epochs, val_metrics)\n",
    "#     plt.title('Training and validation '+ metric)\n",
    "#     plt.xlabel(\"Epochs\")\n",
    "#     plt.ylabel(metric)\n",
    "#     plt.grid()\n",
    "#     plt.legend([\"train_\"+metric, 'val_'+metric])\n",
    "#     plt.show()\n",
    "# plot_metric(history, 'loss')\n",
    "# clf_nn.save_weights(f'./blackboxes/{dataset_name}_tf_nn')\n",
    "# clf_nn.load_weights(f'./blackboxes/{dataset_name}_tf_nn')\n",
    "# clf_nn.trainable = False\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# print(accuracy_score(np.round(clf_nn.predict(X_train)),Y_train))\n",
    "# print(accuracy_score(np.round(clf_nn.predict(X_test)),Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- PCA --------\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 6)\n",
    "pca.fit(X_train)\n",
    "Z_train = pca.transform(X_train)\n",
    "Z_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # -------- VAE --------\n",
    "\n",
    "    # -------- Latent Space --------\n",
    "    X_train = np.hstack((X_train,y_train_pred.reshape(-1,1)))\n",
    "    X_test = np.hstack((X_test,y_test_pred.reshape(-1,1)))\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "    if latent_train:\n",
    "        similarity_KLD = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "        \n",
    "        def compute_similarity_Z(Z, sigma):\n",
    "            D = 1 - F.cosine_similarity(Z[:, None, :], Z[None, :, :], dim=-1)\n",
    "            M = torch.exp((-D**2)/(2*sigma**2))\n",
    "            return M / (torch.ones([M.shape[0],M.shape[1]])*(torch.sum(M, axis = 0)-1)).transpose(0,1)\n",
    "        \n",
    "        def compute_similarity_X(X, sigma, idx_cat=None):\n",
    "            D_class = torch.cdist(X[:,-1].reshape(-1,1),X[:,-1].reshape(-1,1))\n",
    "            X = X[:, :-1]\n",
    "            if idx_cat:\n",
    "                X_cat = X[:, idx_cat]\n",
    "                X_cont = X[:, np.delete(range(X.shape[1]),idx_cat)]\n",
    "                h = X_cat.shape[1]\n",
    "                m = X.shape[1]\n",
    "                D_cont = 1 - F.cosine_similarity(X[:, None, :], X[None, :, :], dim=-1)\n",
    "                D_cat = torch.cdist(X_cat, X_cat, p=0)/h\n",
    "                D = h/m * D_cat + ((m-h)/m) * D_cont + D_class\n",
    "            else:\n",
    "                D_features = 1 - F.cosine_similarity(X[:, None, :], X[None, :, :], dim=-1) \n",
    "                D = D_features + D_class\n",
    "            M = torch.exp((-D**2)/(2*sigma**2))\n",
    "            return M / (torch.ones([M.shape[0],M.shape[1]])*(torch.sum(M, axis = 0)-1)).transpose(0,1)\n",
    "        \n",
    "        def loss_function(X, Z, idx_cat, sigma=1):\n",
    "            Sx = compute_similarity_X(X, sigma, idx_cat)\n",
    "            Sz = compute_similarity_Z(Z, sigma)\n",
    "            loss = similarity_KLD(torch.log(Sx), Sz)\n",
    "            return loss\n",
    "        \n",
    "        class LinearModel(nn.Module):\n",
    "            def __init__(self, input_shape, latent_dim):\n",
    "                super(LinearModel, self).__init__()\n",
    "            \n",
    "                # encoding components\n",
    "                self.fc1 = nn.Linear(input_shape, latent_dim)\n",
    "            \n",
    "            def encode(self, x):\n",
    "                x = self.fc1(x)\n",
    "                return x\n",
    "            \n",
    "            def forward(self, x):\n",
    "                z = self.encode(x)\n",
    "                return z\n",
    "            \n",
    "        # Create Model\n",
    "        model = LinearModel(X_train.shape[1], latent_dim=latent_dim)\n",
    "        \n",
    "        train_dataset = TensorDataset(torch.tensor(X_train).float())\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) \n",
    "        test_dataset = TensorDataset(torch.tensor(X_test).float())\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) \n",
    "    \n",
    "        def check_and_clear(dir_name):\n",
    "            if not os.path.exists(dir_name):\n",
    "                os.mkdir(dir_name)\n",
    "            else:\n",
    "                os.system('rm -r ' + dir_name)\n",
    "                os.mkdir(dir_name)\n",
    "        \n",
    "        check_and_clear('./models/weights')\n",
    "        \n",
    "        model_params = list(model.parameters())\n",
    "        optimizer = torch.optim.Adam(model_params, lr=learning_rate)\n",
    "        \n",
    "        # record training process\n",
    "        epoch_train_losses = []\n",
    "        epoch_test_losses = []\n",
    "        \n",
    "        #validation parameters\n",
    "        epoch = 1\n",
    "        best = np.inf\n",
    "        \n",
    "        # progress bar\n",
    "        pbar = tqdm(bar_format=\"{postfix[0]} {postfix[1][value]:03d} {postfix[2]} {postfix[3][value]:.5f} {postfix[4]} {postfix[5][value]:.5f} {postfix[6]} {postfix[7][value]:d}\",\n",
    "                postfix=[\"Epoch:\", {'value':0}, \"Train Sim Loss\", {'value':0}, \"Test Sim Loss\", {'value':0}, \"Early Stopping\", {\"value\":0}])\n",
    "        \n",
    "        # start training\n",
    "        while epoch <= max_epochs:\n",
    "        \n",
    "            # ------- TRAIN ------- #\n",
    "            # set model as training mode\n",
    "            model.train()\n",
    "            batch_loss = []\n",
    "            \n",
    "            for batch, (X_batch,) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                Z_batch = model(X_batch)  #\n",
    "                loss  = loss_function(X_batch, Z_batch, idx_cat, sigma) \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss.append(loss.item())\n",
    "            \n",
    "            # save result\n",
    "            epoch_train_losses.append(np.mean(batch_loss))\n",
    "            pbar.postfix[3][\"value\"] = np.mean(batch_loss)\n",
    "        \n",
    "            # -------- VALIDATION --------\n",
    "            \n",
    "            # set model as testing mode\n",
    "            model.eval()\n",
    "            batch_loss = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch, (X_batch,) in enumerate(test_loader):\n",
    "                    Z_batch = model(X_batch)\n",
    "                    loss = loss_function(X_batch, Z_batch, idx_cat, sigma)\n",
    "                    batch_loss.append(loss.item())\n",
    "            \n",
    "            # save information\n",
    "            epoch_test_losses.append(np.mean(batch_loss))\n",
    "            pbar.postfix[5][\"value\"] = np.mean(batch_loss)\n",
    "            pbar.postfix[1][\"value\"] = epoch\n",
    "            \n",
    "            if epoch_test_losses[-1] < best:\n",
    "                wait = 0\n",
    "                best = epoch_test_losses[-1]\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), f'./models/weights/LinearTransparent_{dataset_name}.pt')\n",
    "            else:\n",
    "                wait += 1\n",
    "            pbar.postfix[7][\"value\"] = wait\n",
    "            if wait == early_stopping:\n",
    "                break    \n",
    "            epoch += 1\n",
    "            pbar.update()\n",
    "        \n",
    "        model.load_state_dict(torch.load(f'./models/weights/LinearTransparent_{dataset_name}.pt'))\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            Z_train = model(torch.tensor(X_train).float()).cpu().detach().numpy()\n",
    "            Z_test = model(torch.tensor(X_test).float()).cpu().detach().numpy()\n",
    "    \n",
    "        torch.save(model.state_dict(), f'./models/{dataset_name}_latent_{black_box}_{latent_dim}.pt')\n",
    "    \n",
    "    model.load_state_dict(torch.load(f'./models/{dataset_name}_latent_{black_box}_{latent_dim}.pt'))\n",
    "    with torch.no_grad():\n",
    "       model.eval()\n",
    "       Z_train = model(torch.tensor(X_train).float()).cpu().detach().numpy()\n",
    "       Z_test = model(torch.tensor(X_test).float()).cpu().detach().numpy()\n",
    "    \n",
    "    plt.scatter(Z_train[:,0], Z_train[:,1], c=y_train_pred, cmap='coolwarm')\n",
    "    plt.grid()\n",
    "    \n",
    "    w = model.fc1.weight.detach().numpy()\n",
    "    b = model.fc1.bias.detach().numpy()\n",
    "    y_contrib = model.fc1.weight.detach().numpy()[:,-1]\n",
    "    \n",
    "    def compute_cf(q, indexes):\n",
    "       q_pred = predict(q[:-1].reshape(1,-1),return_proba=True)\n",
    "       q_cf = q.copy()\n",
    "       q_cf_preds = []\n",
    "       q_cf_preds.append(float(predict(q_cf[:-1].reshape(1,-1),return_proba=True)))\n",
    "       if q_pred > 0.5:\n",
    "           m = -0.1\n",
    "       else:\n",
    "           m = +0.1\n",
    "       while np.round(q_pred) == np.round(q_cf_preds[-1]):\n",
    "           v = np.array(model(torch.tensor(q_cf).float()).detach().numpy()+m*y_contrib)\n",
    "           c_l = [v[l] - np.sum(q_cf*w[l,:]) - b[l] for l in range(latent_dim)]\n",
    "           M = []\n",
    "           for l in range(latent_dim):\n",
    "               M.append([np.sum(w[k,indexes]*w[l,indexes]) for k in range(latent_dim)])\n",
    "           M = np.vstack(M)\n",
    "           lambda_k = np.linalg.solve(M, c_l)\n",
    "           delta_i = [np.sum(lambda_k*w[:,i]) for i in indexes]\n",
    "           q_cf[indexes] += delta_i\n",
    "           #q_cf = np.clip(q_cf,-1,1)\n",
    "           if float(predict(q_cf[:-1].reshape(1,-1),return_proba=True)) in q_cf_preds:\n",
    "               return q_cf\n",
    "           q_cf_preds.append(float(predict(q_cf[:-1].reshape(1,-1),return_proba=True)))\n",
    "           q_cf[-1] = q_cf_preds[-1]\n",
    "       return q_cf\n",
    "    \n",
    "    from itertools import combinations\n",
    "    from scipy.spatial.distance import cdist\n",
    "    \n",
    "    d_dist = []\n",
    "    d_impl = []\n",
    "    d_count = []\n",
    "    d_adv = []\n",
    "    num = []\n",
    "    div_dist = []\n",
    "    div_count = []\n",
    "    \n",
    "    for idx in tqdm(range(n)):\n",
    "       q = X_test[idx,:].copy()\n",
    "       q_pred = predict(q[:-1].reshape(1,-1),return_proba=False)\n",
    "       q_cfs = []\n",
    "       l_i = []\n",
    "       l_f = []\n",
    "    \n",
    "       for indexes in list(combinations(list(range(X_train.shape[1]-1)),1)):    \n",
    "           q_cf = compute_cf(q, list(indexes))\n",
    "           q_cf_pred = predict(q_cf[:-1].reshape(1,-1),return_proba=True)\n",
    "           if q_pred:\n",
    "               if q_cf_pred<0.5:\n",
    "                   q_cfs.append(q_cf)\n",
    "           else:\n",
    "               if q_cf_pred>0.5:\n",
    "                   q_cfs.append(q_cf) \n",
    "    \n",
    "       for indexes in list(combinations(list(range(X_train.shape[1]-1)),2)):    \n",
    "           q_cf = compute_cf(q, list(indexes))\n",
    "           q_cf_pred = predict(q_cf[:-1].reshape(1,-1),return_proba=True)\n",
    "           if q_pred:\n",
    "               if q_cf_pred<0.5:\n",
    "                   q_cfs.append(q_cf)\n",
    "           else:\n",
    "               if q_cf_pred>0.5:\n",
    "                   q_cfs.append(q_cf) \n",
    "           l_i.append([list(indexes),q_cf_pred])\n",
    "       r = np.argsort(np.stack(np.array(l_i,dtype=object)[:,1]).ravel())[-10:]\n",
    "       l_i = np.array(l_i,dtype=object)[r,0]\n",
    "    \n",
    "       while len(l_i[0])<6:\n",
    "           for e in l_i:\n",
    "               for i in list(np.delete(range(X_train.shape[1]-1),e)):\n",
    "                   q_cf = compute_cf(q, e+[i])\n",
    "                   q_cf_pred = predict(q_cf[:-1].reshape(1,-1),return_proba=True)\n",
    "                   if q_pred:\n",
    "                       if q_cf_pred<0.5:\n",
    "                           q_cfs.append(q_cf)\n",
    "                   else:\n",
    "                       if q_cf_pred>0.5:\n",
    "                           q_cfs.append(q_cf) \n",
    "                   l_f.append([e+[i],q_cf_pred])\n",
    "           r = np.argsort(np.stack(np.array(l_f,dtype=object)[:,1]).ravel())[-10:]\n",
    "           l_f = np.array(l_f,dtype=object)[r,0]\n",
    "           l_i = l_f.copy()\n",
    "           l_f = []\n",
    "       \n",
    "       if len(q_cfs)<1:\n",
    "           continue\n",
    "       else:\n",
    "           q_cfs = np.vstack(q_cfs)\n",
    "           if dataset_name == 'fico':\n",
    "               d_dist.append(np.min(cdist(q_cfs[:,:-1],q[:-1].reshape(1,-1))))\n",
    "               d_impl.append(np.min(cdist(q_cfs[:,:-1],X_train[:,:-1])))\n",
    "               d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))\n",
    "               r = np.argsort(cdist(q_cfs[:,:-1],X_train[:,:-1]),axis=1)[:,:10]\n",
    "               d_adv.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==q_pred) for i in range(q_cfs.shape[0])])))\n",
    "               num.append(len(q_cfs))\n",
    "               div_dist.append(1/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1],q_cfs[:,:-1])))\n",
    "               div_count.append((X_train.shape[1]-1)/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))\n",
    "           elif dataset_name == 'adult':\n",
    "               d_dist.append(np.min(cdist(q_cfs[:,[2,3,4,5,6]],q[[2,3,4,5,6]].reshape(1,-1),metric='hamming') + cdist(q_cfs[:,[0,1]],q[[0,1]].reshape(1,-1),metric='euclidean')))\n",
    "               d_impl.append(np.min(cdist(q_cfs[:,[2,3,4,5,6]],X_train[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cfs[:,[0,1]],X_train[:,[0,1]],metric='euclidean')))\n",
    "               d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))\n",
    "               r = np.argsort(cdist(q_cfs[:,[2,3,4,5,6]],X_train[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cfs[:,[0,1]],X_train[:,[0,1]],metric='euclidean'),axis=1)[:,:10]\n",
    "               d_adv.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==q_pred) for i in range(q_cfs.shape[0])])))\n",
    "               num.append(len(q_cfs))\n",
    "               div_dist.append(np.mean(cdist(q_cfs[:,[2,3,4,5,6]],q_cfs[:,[2,3,4,5,6]],metric='hamming') + cdist(q_cfs[:,[0,1]],q_cfs[:,[0,1]],metric='euclidean')))\n",
    "               div_count.append(X_train.shape[1]-1/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))\n",
    "           elif dataset_name == 'compas':\n",
    "               d_dist.append(np.min(cdist(q_cfs[:,13:-1],q[13:-1].reshape(1,-1),metric='hamming') + cdist(q_cfs[:,:13],q[:13].reshape(1,-1),metric='euclidean')))\n",
    "               d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))\n",
    "               d_impl.append(np.min(cdist(q_cfs[:,13:-1],X_train[:,13:-1],metric='hamming') + cdist(q_cfs[:,:13],X_train[:,:13],metric='euclidean')))\n",
    "               r = np.argsort(cdist(q_cfs[:,13:-1],X_train[:,13:-1],metric='hamming') + cdist(q_cfs[:,:13],X_train[:,:13],metric='euclidean'),axis=1)[:,:10]\n",
    "               d_adv.append(np.mean(np.array([np.mean(predict(X_train[r,:-1][i,:])==q_pred) for i in range(q_cfs.shape[0])])))\n",
    "               num.append(len(q_cfs))\n",
    "               div_dist.append(np.mean(cdist(q_cfs[:,13:-1],q_cfs[:,13:-1],metric='hamming') + cdist(q_cfs[:,:13],q_cfs[:,:13],metric='euclidean')))\n",
    "               div_count.append(X_train.shape[1]-1/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))\n",
    "           elif dataset_name == 'german':\n",
    "               d_dist.append(np.min(cdist(q_cfs[:,3:-1],q[3:-1].reshape(1,-1),metric='hamming') + cdist(q_cfs[:,:3],q[:3].reshape(1,-1),metric='euclidean')))\n",
    "               d_impl.append(np.min(cdist(q_cfs[:,3:-1],X_train[:,3:-1],metric='hamming') + cdist(q_cfs[:,:3],X_train[:,:3],metric='euclidean')))\n",
    "               d_count.append(np.min(np.sum(q_cfs[:,:-1]!=q[:-1],axis=1)))\n",
    "               num.append(len(q_cfs))\n",
    "               div_dist.append(np.mean(cdist(q_cfs[:,3:-1],q_cfs[:,3:-1],metric='hamming') + cdist(q_cfs[:,:3],q_cfs[:,:3],metric='euclidean')))\n",
    "               div_count.append(X_train.shape[1]-1/(q_cfs.shape[0]**2)*np.sum(cdist(q_cfs[:,:-1], q_cfs[:,:-1],metric='hamming')))\n",
    "\n",
    "    d[dataset_name][bb_name][str(latent_dim)]['d_dist_mean'] = np.mean(np.array(d_dist))\n",
    "    d[dataset_name][bb_name][str(latent_dim)]['d_dist_std'] = np.std(np.array(d_dist))\n",
    "    d[dataset_name][bb_name][str(latent_dim)]['d_count_mean'] = np.mean(np.array(d_count))\n",
    "    d[dataset_name][bb_name][str(latent_dim)]['d_count_std'] = np.std(np.array(d_count))\n",
    "    d[dataset_name][bb_name][str(latent_dim)]['d_impl_mean'] = np.mean(np.array(d_impl)) \n",
    "    d[dataset_name][bb_name][str(latent_dim)]['d_impl_std'] = np.std(np.array(d_impl))\n",
    "    d[dataset_name][bb_name][str(latent_dim)]['d_advs_mean'] = np.mean(np.array(d_adv)) \n",
    "    d[dataset_name][bb_name][str(latent_dim)]['d_advs_std'] = np.std(np.array(d_adv))\n",
    "    d[dataset_name][bb_name][str(latent_dim)]['div_count_mean'] = np.mean(np.array(div_dist)) \n",
    "    d[dataset_name][bb_name][str(latent_dim)]['div_count_std'] = np.std(np.array(div_dist))\n",
    "    d[dataset_name][bb_name][str(latent_dim)]['div_dist_mean'] = np.mean(np.array(div_count)) \n",
    "    d[dataset_name][bb_name][str(latent_dim)]['div_dist_std'] = np.std(np.array(div_count))\n",
    "    pickle.dump(d, open(f'./{dataset_name}_{black_box}_ablation_results.p','wb'))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cp-ils')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d28ae43ea26f7880cf7f402099a17e07ed2d1f3edc5ed1e0381edda60db1f0c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
